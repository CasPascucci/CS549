{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2910afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46864/1901847722.py:34: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data2['result'] = data2['type'].replace(mapping2).astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Data Prep. Data Head:\n",
      "                                                 url  result  char  queries  \\\n",
      "0  https://www.amazon.com/Punch-Vincent-Gale/dp/B...       0    55        0   \n",
      "1  https://www.startreklinks.net/series-movies/vo...       0    56        0   \n",
      "2  http://torcache.net/torrent/00611B9CA7EDC70114...       0   109        1   \n",
      "3  https://www.thirdworldtraveler.com/American_Em...       0    76        0   \n",
      "4        syndicalist.org/archives/llr14-24/14f.shtml       0    43        0   \n",
      "\n",
      "   num_subdomains   entropy  \n",
      "0               2  4.738013  \n",
      "1               2  4.150319  \n",
      "2               1  4.876598  \n",
      "3               2  4.298520  \n",
      "4               1  4.332787  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import cuml.accel\n",
    "cuml.accel.install()\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import urllib.parse\n",
    "import math # Required for entropy calculation\n",
    "from collections import Counter # Required for entropy calculation\n",
    "\n",
    "# --- Entropy Calculation Function ---\n",
    "def calculate_entropy(s):\n",
    "    \"\"\"Calculates the Shannon entropy of a string.\"\"\"\n",
    "    if not s:\n",
    "        return 0\n",
    "    # Count character frequencies\n",
    "    counts = Counter(s)\n",
    "    # Calculate probabilities\n",
    "    probabilities = [float(count) / len(s) for count in counts.values()]\n",
    "    # Calculate entropy: H = -sum(p * log2(p))\n",
    "    entropy = -sum(p * math.log2(p) for p in probabilities)\n",
    "    return entropy\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "data1 = pd.read_csv('urldata.csv')\n",
    "data2 = pd.read_csv('malicious_phish.csv')\n",
    "\n",
    "# Map types to binary result\n",
    "mapping = {'phishing' : 'malicious', 'defacement' : 'malicious','malware' : 'malicious',}\n",
    "data2['type'] = data2['type'].replace(mapping)\n",
    "mapping2 = {'malicious': 1, 'benign': 0}\n",
    "data2['result'] = data2['type'].replace(mapping2).astype(int)\n",
    "\n",
    "# Drop unused columns and combine\n",
    "data1.drop(['Unnamed: 0', 'label'],axis=1, inplace=True)\n",
    "data2.drop(['type'],axis=1, inplace=True)\n",
    "data = pd.concat([data1, data2],ignore_index=True)\n",
    "data.drop_duplicates(inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# Under-sampling to balance classes\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "x_url = data[['url']]\n",
    "y = data['result']\n",
    "x_resampled, y_resampled = rus.fit_resample(x_url,y)\n",
    "data = pd.concat([x_resampled,y_resampled],axis=1)\n",
    "data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# --- Feature Engineering (4 Features: char, queries, num_subdomains, entropy) ---\n",
    "data['char'] = data['url'].str.len()\n",
    "\n",
    "def count_query_params(url):\n",
    "    try:\n",
    "        query_string = urllib.parse.urlparse(url).query\n",
    "        if not query_string:\n",
    "            return 0\n",
    "        return len(query_string.split('&'))\n",
    "    except:\n",
    "        return 0\n",
    "data['queries'] = data['url'].apply(count_query_params).astype(int)\n",
    "\n",
    "def count_subdomains(url):\n",
    "    try:\n",
    "        if not url.startswith(('http://', 'https://')):\n",
    "            url = 'http://' + url\n",
    "        netloc = urllib.parse.urlparse(url).netloc\n",
    "        hostname = netloc.split(':')[0]\n",
    "        return hostname.count('.')\n",
    "    except:\n",
    "        return 0\n",
    "data['num_subdomains'] = data['url'].apply(count_subdomains)\n",
    "\n",
    "# NEW FEATURE: Entropy of the URL string (replaces 'has_at_symbol')\n",
    "data['entropy'] = data['url'].apply(calculate_entropy)\n",
    "\n",
    "print('Finished Data Prep. Data Head:')\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa4f6058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 506254, Validation samples: 63282, Test samples: 63282\n"
     ]
    }
   ],
   "source": [
    "# --- Train/Val/Test Split ---\n",
    "# Feature list updated to include 'entropy'\n",
    "x = data[['char', 'queries', 'num_subdomains', 'entropy']]\n",
    "y = data['result']\n",
    "\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(x, y, test_size=0.2, random_state=42, stratify=y)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "print(f\"Train samples: {len(x_train)}, Validation samples: {len(x_val)}, Test samples: {len(x_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8666120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Scaled.\n"
     ]
    }
   ],
   "source": [
    "# --- Scaling (Essential for RBF Kernel) ---\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_val_scaled = scaler.transform(x_val)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "print('Data Scaled.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394be684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Training: SVC RBF Kernel Hyperparameter Tuning (C values) ---\n",
    "\n",
    "C_values = [0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0] \n",
    "gamma_values = ['auto','scale', 0.01, 0.1, 1.0, 10.0]\n",
    "print(\"\\n--- SVC RBF Kernel Hyperparameter Tuning (C values) ---\")\n",
    "\n",
    "best_macro_f1 = 0\n",
    "best_c = 0\n",
    "best_gamma = 0\n",
    "\n",
    "for C_val in C_values:\n",
    "    # Train the RBF Model with the current C value\n",
    "    rbf_svm = SVC(kernel='rbf', C=C_val, gamma='scale', random_state=42,cache_size=4096)\n",
    "    rbf_svm.fit(X=x_train_scaled, y=y_train)\n",
    "\n",
    "    # Predict on Validation Set\n",
    "    val_pred = rbf_svm.predict(x_val_scaled)\n",
    "    \n",
    "    # Calculate and print key metrics\n",
    "    val_accuracy = accuracy_score(y_val, val_pred)\n",
    "    report = classification_report(y_val, val_pred, output_dict=True)\n",
    "    val_macro_f1 = report['macro avg']['f1-score']\n",
    "    \n",
    "    print(f\"\\nResults for C = {C_val}:\")\n",
    "    print(f\"  Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"  Validation Macro F1: {val_macro_f1:.4f}\")\n",
    "    print(\"---------------------------------------\")\n",
    "    \n",
    "    if val_macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = val_macro_f1\n",
    "        best_c = C_val\n",
    "\n",
    "print(f\"\\nBest C value (by Validation Macro F1): {best_c}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FINAL TEST RESULTS (Optimal C) ---\n",
      "Accuracy: 0.47386302582092854\n",
      "Confusion Matrix:\n",
      " [[14932 16709]\n",
      " [16586 15055]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.47      0.47     31641\n",
      "           1       0.47      0.48      0.47     31641\n",
      "\n",
      "    accuracy                           0.47     63282\n",
      "   macro avg       0.47      0.47      0.47     63282\n",
      "weighted avg       0.47      0.47      0.47     63282\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Final Evaluation (Run this after choosing the Best C) ---\n",
    "# Replace best_c_value with the value determined in the cell above.\n",
    "best_c_value = 0.1 # Placeholder: Replace with the actual best C from the tuning loop\n",
    "\n",
    "final_rbf_svm = SVC(kernel='rbf', C=best_c_value, gamma='scale', random_state=42, cache_size=4096,)\n",
    "final_rbf_svm.fit(X=x_train_scaled, y=y_train)\n",
    "\n",
    "# Test Set Evaluation (Use this only for your final report)\n",
    "test_pred = final_rbf_svm.predict(x_test_scaled)\n",
    "print(\"\\n--- FINAL TEST RESULTS (Optimal C) ---\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, test_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, test_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, test_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
