{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Linear Regression (40 points)\n",
    "\n",
    "The goal of this section is to practice the Python implementation of two methods for training Linear Regression models: **normal equations** and **gradient descent**. See detailed instructions below. \n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "The task is to build a linear regression model that predicts the GPAs of university students from two features, Math SAT and Verb SAT. \n",
    "- Task 1) Train the model using Normal Equation method. (12 pts)\n",
    "- Task 2) Train the model using Gradient Descent method. (25 pts)\n",
    "- Task 3) Play around with different learning rate $\\alpha$s. (3 pts)\n",
    "\n",
    "## Datasets\n",
    "The file *sat_gpa.csv* contains all training and testing data. It has 105 rows and 3 columns. Each row is the record of a student. The three columns are <u>Math SAT score</u>, <u>Verb SAT score</u>, and <u>University GPA</u>. The first two columns are the features, and the third is the output. All data points are used as the training set.\n",
    "\n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv # Used for computing the inverse of matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Use `pip install matplotlib` in command line if matplotlib is not installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data \n",
    "data = np.loadtxt(open('sat_gpa.csv'), delimiter=',')\n",
    "print('shape of original data:', data.shape) # Check if data is 105 by 3\n",
    "\n",
    "# Normalize data\n",
    "data_norm = data / data.max(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Task 1 (12 points): Implement the Normal Equation method. \n",
    "\n",
    "Implement the Normal Equation method for linear regression: $\\theta = (X^T X)^{-1}X^T y$\n",
    "\n",
    "Use the learned $\\theta$ to make predictions: $\\hat{y} = X\\theta$\n",
    "\n",
    "Compute the mean squared error of the model: $MSE = 1/n \\sum_i (\\hat{y}^{(i)} - y^{(i)})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix X and y (4 pts)\n",
    "# X has three columns: \n",
    "#   - The first column contain all 1s, which is for the intercept\n",
    "#   - The second and third columns contain features, i.e., the 1st and 2nd columns of data_norm\n",
    "# y has one column, i.e., the 3rd column of data_norm\n",
    "\n",
    "X = np.ones_like(data_norm)\n",
    "#### START YOUR CODE ####\n",
    "X[:, 1:3] = None\n",
    "y = None\n",
    "#### END YOUR CODE ####\n",
    "\n",
    "\n",
    "# Compute theta using normal equation method (4 pts)\n",
    "# Hint: use the inv() function imported from numpy.linalg\n",
    "#### START YOUR CODE ####\n",
    "theta_method1 = None\n",
    "#### END YOUR CODE ####\n",
    "\n",
    "\n",
    "# Use the theta obtained to make predictions and compute the residuals (4 pts)\n",
    "# Hint: use numpy.dot() and numpy.sum(), and avoid using for loops\n",
    "#### START YOUR CODE ####\n",
    "y_hat = None\n",
    "MSE1 = None\n",
    "#### END YOUR CODE ####\n",
    "\n",
    "# Compute residuals\n",
    "\n",
    "\n",
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "print('Theta obtained from normal equation:', theta_method1)\n",
    "print('Mean Squared Error: ', MSE1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected ouput\n",
    "\n",
    "Theta obtained from normal equation: | [-0.06234478  0.62017319  0.43647674]\\\n",
    "Mean Squared Error:  0.0072290203647900324\n",
    "\n",
    "---\n",
    "\n",
    "### Task 2 (25 points): Implement the Gradient Descent method for linear regression.\n",
    "\n",
    "The cost function: $J(\\theta_0, \\theta_1, \\theta_2) = \\frac{1}{2m}\\sum_i (\\hat{y}^{(i)} - y^{(i)})^2 = \\frac{1}{2m}\\sum_i (\\theta_0 + \\theta_1 x_1^{(i)} + \\theta_2 x_2^{(i)} - y^{(i)})^2$\n",
    "\n",
    "Gradients w.r.t. parameters: $\\frac{\\partial J}{\\partial \\theta} = \\begin{cases}\\frac{\\partial J}{\\partial \\theta_0}\\\\ \\frac{\\partial J}{\\partial \\theta_1}\\\\ \\frac{\\partial J}{\\partial \\theta_2}\\\\ \\end{cases} = \\begin{cases}\\frac{1}{m}\\sum_i (\\hat{y}^{(i)} - y^{(i)})\\\\ \\frac{1}{m}\\sum_i (\\hat{y}^{(i)} - y^{(i)})x_1^{(i)}\\\\ \\frac{1}{m}\\sum_i (\\hat{y}^{(i)} - y^{(i)})x_2^{(i)}\\\\\\end{cases}$\n",
    "\n",
    "The formula to update parameters at each iteration: $\\theta := \\theta - \\alpha * \\frac{\\partial J}{\\partial \\theta}$\n",
    "\n",
    "Note that $X$, $y$, and $\\theta$ are all vectors (numpy arrays), and thus the operations above should be implemented in a vectorized fashion. Use `numpy.sum()`, `numpy.dot()` and other vectorized functions, and avoid writing `for` loops in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the gradientDescent function\n",
    "def gradientDescent(X, y, theta, alpha, num_iters):\n",
    "    '''\n",
    "    Params\n",
    "        X - Shape: (m,3); m is the number of data examples\n",
    "        y - Shape: (m,)\n",
    "        theta - Shape: (3,)\n",
    "        num_iters - Maximum number of iterations\n",
    "    Return\n",
    "        A tuple: (theta, MSE, cost_array)\n",
    "        theta - the learned model parameters\n",
    "        MSE - mean squared error\n",
    "        cost_array - stores the cost value of each iteration. Its shape is (num_iters,)\n",
    "    '''\n",
    "    m = len(y)\n",
    "    cost_array =[]\n",
    "\n",
    "    for i in range(0, num_iters):\n",
    "        #### START YOUR CODE ####\n",
    "        # Make predictions (1 pts)\n",
    "        # Shape of y_hat: m by 1\n",
    "        y_hat = None\n",
    "        \n",
    "        # Compute the difference between prediction (y_hat) and ground truth label (y) (1 pts)\n",
    "        diff = None\n",
    "\n",
    "        # Compute the cost (1 pt)\n",
    "        # Hint: Use the diff computed above\n",
    "        cost = None\n",
    "        cost_array.append(cost)\n",
    "\n",
    "        # Compute gradients (10 pts)\n",
    "        # Hint: Use the diff computed above\n",
    "        # Hint: Shape of gradients is the same as theta\n",
    "        gradients = None\n",
    "\n",
    "        # Update theta (10 pts)\n",
    "        theta = None\n",
    "        #### END YOUR CODE ####\n",
    "    \n",
    "    # Compute residuals (2 pts)\n",
    "    # Hint: Should use the same code as Task 1\n",
    "    #### START YOUR CODE ####\n",
    "    y_hat = None\n",
    "    MSE = None\n",
    "    #### END YOUR CODE ####\n",
    "\n",
    "    return theta, MSE, cost_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This cell is to evaluate the gradientDescent function implemented above\n",
    "\n",
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "# Define learning rate and maximum iteration number\n",
    "ALPHA = 0.04\n",
    "MAX_ITER = 600\n",
    "\n",
    "# Initialize theta to [0,0,0]\n",
    "theta = np.zeros(3)\n",
    "theta_method2, MSE2, cost_array = gradientDescent(X, y, theta, ALPHA, MAX_ITER)\n",
    "\n",
    "print('Theta obtained from gradient descent:', theta_method2)\n",
    "print('Mean Squared Error (MSE): ', MSE2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "\n",
    "Theta obtained from gradient descent: | [0.29911574 0.32224209 0.31267172]\\\n",
    "Mean Squared Error (MSE):  0.008230095794638667\n",
    "\n",
    "---\n",
    "\n",
    "### Task 3 (3 points): Play around with learning rates. \n",
    "\n",
    "Plot the cost against iteration number. This is a common method of examining the performance of gradient descent.\n",
    "\n",
    "Try different values of learning rate, for example, $\\alpha=\\{0.01, 0.005, 0.001\\}$, and see how the cost curves change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### START YOUR CODE ####\n",
    "alpha = None\n",
    "#### END YOUR CODE ####\n",
    "\n",
    "theta = np.zeros(3)\n",
    "_, _, cost_array = gradientDescent(X, y, theta, alpha, MAX_ITER)\n",
    "\n",
    "plt.plot(range(0,len(cost_array)), cost_array);\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('cost')\n",
    "plt.title('alpha = {}'.format(alpha))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Data Preprocessing ane Evaluation\n",
    "\n",
    "You will practice on common data preprocessing and evaluation tasks in this section.\n",
    "\n",
    "- Task 1: 5 points\n",
    "- Task 2: 8 points\n",
    "- Task 3: 8 points\n",
    "- Task 4: 9 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (5 pts): One-Hot Encoding\n",
    "\n",
    "Implement a function one_hot_encode that takes a list of categorical values and returns their one-hot encoded representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "### START YOUR CODE ###\n",
    "def one_hot_encode(categories):\n",
    "    \"\"\"\n",
    "    Convert categorical values to one-hot encoding.\n",
    "    \n",
    "    Args:\n",
    "        categories (list of str): A list of categorical values.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: A one-hot encoded matrix.\n",
    "    \"\"\"\n",
    "    unique_categories = None  # TODO: Get unique categories (2 pts)\n",
    "    encoding = None  # TODO: Implement one-hot encoding logic (3 pts)\n",
    "    return encoding\n",
    "### END YOUR CODE ###\n",
    "\n",
    "#### DO NOT CHANGE THE CODE BELOW ####\n",
    "categories = [\"red\", \"blue\", \"green\", \"blue\", \"red\"]\n",
    "one_hot_encode(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "<!-- |&nbsp;|&nbsp; |           -->\n",
    "\n",
    "[[1., 0., 0.],<br>\n",
    " [0., 1., 0.],<br>\n",
    " [0., 0., 1.],<br>\n",
    " [0., 1., 0.],<br>\n",
    " [1., 0., 0.]]<br>\n",
    "\n",
    "or \n",
    "[[0. 1. 0.],<br>\n",
    " [1. 0. 0.],<br>\n",
    " [0. 0. 1.],<br>\n",
    " [1. 0. 0.],<br>\n",
    " [0. 1. 0.]]<br>\n",
    " \n",
    " or \n",
    "[[1. 0. 0.],<br>\n",
    " [0. 0. 1.],<br>\n",
    " [0. 1. 0.],<br>\n",
    " [0. 0. 1.],<br>\n",
    " [1. 0. 0.]<br>\n",
    " ]\n",
    " \n",
    " or \n",
    "[[0. 1. 0.],<br>\n",
    " [0. 0. 1.],<br>\n",
    " [1. 0. 0.],<br>\n",
    " [0. 0. 1.]],<br>\n",
    " [0. 1. 0.]]<br>\n",
    "  \n",
    "  or \n",
    "[[0. 0. 1.],<br>\n",
    " [1. 0. 0.],<br>\n",
    " [0. 1. 0.],<br>\n",
    " [1. 0. 0.],<br>\n",
    " [0. 0. 1.]]<br>\n",
    " ]\n",
    "\n",
    "or \n",
    "[[0. 0. 1.],<br>\n",
    " [0. 1. 0.],<br>\n",
    " [1. 0. 0.],<br>\n",
    " [0. 1. 0.],<br>\n",
    " [0. 0. 1.]]\n",
    " ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (8 pts): One-vs-One (OvO) Classification\n",
    "\n",
    "One-vs-One (OvO) classification strategy involves training a separate classifier for every possible pair of classes.\n",
    "\n",
    "In this task, we train a separate binary classifier for each possible pair of classes. With three classes (0, 1, 2), we train three classifiers:\n",
    "\n",
    "Classifier 1: Class 0 vs. Class 1\n",
    "Classifier 2: Class 0 vs. Class 2\n",
    "Classifier 3: Class 1 vs. Class 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def one_vs_one(X, y):\n",
    "    \"\"\"\n",
    "    Implements One-vs-One classification by training a binary classifier for each class pair.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Feature matrix.\n",
    "        y (np.ndarray): Target values (three classes: 0, 1, 2).\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary where keys are class pairs (tuples) and values are trained classifiers.\n",
    "    \"\"\"\n",
    "    classifiers = {}  # Dictionary to store classifiers\n",
    "    classes = {0, 1, 2}  # Three unique class labels\n",
    "\n",
    "    for class1, class2 in combinations(classes, 2):\n",
    "        ### START YOUR CODE ###\n",
    "        # TODO: Select only data belonging to class1 or class2\n",
    "        mask = None  #2pts\n",
    "        X_pair, y_pair = None, None   #2pts\n",
    "\n",
    "        # TODO: Convert labels to binary (1 for class1, 0 for class2)\n",
    "        y_pair = None   #2pts\n",
    "\n",
    "        # TODO: Train a logistic regression classifier\n",
    "        model = LogisticRegression()   \n",
    "        model.fit(None, None)  # Train model #2pts\n",
    "\n",
    "        # Store trained model\n",
    "        classifiers[(class1, class2)] = model\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    return classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data: DO NOT CHANGE THE CODE BELOW ###\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset from CSV\n",
    "csv_path = \"data.csv\"  # Ensure the file is in the same directory\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Extract features and labels\n",
    "X = df[[\"Feature1\", \"Feature2\"]].values\n",
    "y = df[\"Class\"].values\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation: DO NOT CHANGE THE CODE BELOW ###\n",
    "# Train OvO classifiers\n",
    "ovo_models = one_vs_one(X_train, y_train)\n",
    "\n",
    "# Function to make predictions using OvO classifiers\n",
    "def ovo_predict(X_test, ovo_models):\n",
    "    \"\"\"\n",
    "    Predict class labels using One-vs-One classifiers via majority voting.\n",
    "    \"\"\"\n",
    "    votes = {i: np.zeros(len(X_test)) for i in range(3)}  # Vote count per class\n",
    "    for (class1, class2), model in ovo_models.items():\n",
    "        preds = model.predict(X_test)\n",
    "        for i, pred in enumerate(preds):\n",
    "            if pred == 1:\n",
    "                votes[class1][i] += 1\n",
    "            else:\n",
    "                votes[class2][i] += 1\n",
    "    return np.array([max(votes, key=lambda c: votes[c][i]) for i in range(len(X_test))])\n",
    "\n",
    "# Evaluate OvO model\n",
    "y_pred_ovo = ovo_predict(X_test, ovo_models)\n",
    "accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
    "print(f\"One-vs-One Accuracy: {accuracy_ovo:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "One-vs-One Accuracy: 0.96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 (8 pts): One-vs-All (OvA) Classification\n",
    "\n",
    "In One-vs-All (OvA) classification, we train a separate binary classifier for each class, treating that class as positive (1) and all other classes as negative (0).\n",
    "\n",
    "For a dataset with three classes (0, 1, 2), we train three classifiers:\n",
    "\n",
    "Classifier 1: Class 0 vs. (Class 1 + Class 2)\n",
    "Classifier 2: Class 1 vs. (Class 0 + Class 2)\n",
    "Classifier 3: Class 2 vs. (Class 0 + Class 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def one_vs_all(X, y):\n",
    "    \"\"\"\n",
    "    Implements One-vs-All classification by training a binary classifier for each class.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Feature matrix.\n",
    "        y (np.ndarray): Target values (three classes: 0, 1, 2).\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary where keys are individual class labels and values are trained classifiers.\n",
    "    \"\"\"\n",
    "    classifiers = {}  # Dictionary to store classifiers\n",
    "    classes = {0, 1, 2}  # Three unique class labels\n",
    "\n",
    "    for class_label in classes:\n",
    "        ### START YOUR CODE ###\n",
    "        # TODO: Convert labels to binary (1 for class_label, 0 for all other classes)\n",
    "        y_binary = None  #6pts\n",
    "\n",
    "        # TODO: Train a logistic regression classifier\n",
    "        model = LogisticRegression()    \n",
    "        model.fit(None, None)  # Train model #2pts\n",
    "\n",
    "        # Store trained model\n",
    "        classifiers[class_label] = model\n",
    "        ### END YOUR CODE ###\n",
    "\n",
    "    return classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data: DO NOT CHANGE THE CODE BELOW ###\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset from CSV\n",
    "csv_path = \"data.csv\"  # Ensure the file is in the same directory\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Extract features and labels\n",
    "X = df[[\"Feature1\", \"Feature2\"]].values\n",
    "y = df[\"Class\"].values\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation: DO NOT CHANGE THE CODE BELOW ###\n",
    "# Train OvA classifiers\n",
    "ova_models = one_vs_all(X_train, y_train)\n",
    "\n",
    "# Function to make predictions using OvA classifiers\n",
    "def ova_predict(X_test, ova_models):\n",
    "    \"\"\"\n",
    "    Predict class labels using One-vs-All classifiers by selecting the class with the highest probability.\n",
    "    \"\"\"\n",
    "    probs = np.zeros((len(X_test), 3))  # Store probabilities for each class\n",
    "    for class_label, model in ova_models.items():\n",
    "        probs[:, class_label] = model.predict_proba(X_test)[:, 1]  # Probability of being class_label\n",
    "\n",
    "    return np.argmax(probs, axis=1)  # Choose class with highest probability\n",
    "\n",
    "# Evaluate OvA model\n",
    "y_pred_ova = ova_predict(X_test, ova_models)\n",
    "accuracy_ova = accuracy_score(y_test, y_pred_ova)\n",
    "print(f\"One-vs-All Accuracy: {accuracy_ova:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "One-vs-All Accuracy: 0.93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (9 pts): Implement Cross-Validation\n",
    "Cross-validation is a model evaluation technique used to assess the performance of a model on unseen data. One common method is $k$-fold cross-validation, where the dataset is split into $k$ equal-sized folds. The model is trained on $k-1$ folds and tested on the remaining fold, and this process is repeated $k$ times.\n",
    "\n",
    "Documentation from sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def k_fold_cross_validation(X, y, k=5):\n",
    "    \"\"\"\n",
    "    Performs k-fold cross-validation for a logistic regression model.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Feature matrix.\n",
    "        y (np.ndarray): Target labels.\n",
    "        k (int): Number of folds.\n",
    "\n",
    "    Returns:\n",
    "        list: A list containing accuracy scores for each fold.\n",
    "    \"\"\"\n",
    "    accuracies = []  # Store accuracy for each fold\n",
    "\n",
    "    ### START YOUR CODE ###\n",
    "    # TODO: Initialize KFold in sklearb with k splits, shuffle=True, random_state=42, please read KFold documentation first\n",
    "    kf = None  # 3 pts\n",
    "\n",
    "    # TODO: Iterate through the folds and train a model for each\n",
    "    for train_index, test_index in None:  # TODO: Use kf.split() # 1 pt\n",
    "        X_train, X_test = None, None  # TODO: Split data # 1 pts\n",
    "        y_train, y_test = None, None  # TODO: Split labels # 1 pts\n",
    "\n",
    "        # TODO: Initialize and train a logistic regression model\n",
    "        model = LogisticRegression()  \n",
    "        model.fit(None, None)  # Train model on training data  # 1 pt\n",
    "\n",
    "        # TODO: Predict on the test fold\n",
    "        y_pred = None  # 1 pt\n",
    "\n",
    "        # TODO: Compute accuracy and store it\n",
    "        acc = None  # 1 pt\n",
    "        accuracies.append(acc)\n",
    "    ### END YOUR CODE ###\n",
    "\n",
    "    return accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation: DO NOT CHANGE THE CODE BELOW ###\n",
    "# Load dataset\n",
    "csv_path = \"data2.csv\"  # Ensure the file is in the same directory\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Extract features and labels\n",
    "X = df[[\"Feature1\", \"Feature2\"]].values\n",
    "y = df[\"Class\"].values\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "cv_accuracies = k_fold_cross_validation(X, y, k=5)\n",
    "\n",
    "# Print results\n",
    "print(f\"Cross-Validation Accuracies: {cv_accuracies}\")\n",
    "print(f\"Mean Accuracy: {np.mean(cv_accuracies):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "Cross-Validation Accuracies: [0.8, 0.875, 0.925, 0.8, 0.925]\n",
    "Mean Accuracy: 0.86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: SVM (30 points)\n",
    "- Task 1: 9 points\n",
    "- Task 2: 9 points\n",
    "- Task 3:  3 points\n",
    "- Task 4: 9 points\n",
    "\n",
    "In this assignment, we will build a \"toy\" SVM model using a mini dataset step by step.\n",
    "\n",
    "For each **task** cell that requires your completion, you can run the **evaluation** cell right after it to check if your answer correct.\n",
    "The output of the evaluation cell should be the same as the \"expected output\" provided. (Some mismatch in the last digit of floating numbers is tolerable)\n",
    "\n",
    "---\n",
    "# Install dependencies\n",
    "\n",
    "**quadprog** is a Python package for solving quadratic programming problems. You can install it using the following command:\n",
    "```\n",
    "pip install quadprog\n",
    "```\n",
    "Note: Windows users may need to install Visual C++ 14 first (https://visualstudio.microsoft.com/visual-cpp-build-tools/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quadprog\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# The helper function. Dot not change it\n",
    "def quadprog_solve_qp(P, q, G=None, h=None, A=None, b=None):\n",
    "    qp_G = .5 * (P + P.T)   # make sure P is symmetric\n",
    "    qp_a = -q\n",
    "    if A is not None:\n",
    "        qp_C = -np.vstack([A, G]).T\n",
    "        qp_b = -np.hstack([b, h])\n",
    "        meq = A.shape[0]\n",
    "    else:  # no equality constraint\n",
    "        qp_C = -G.T\n",
    "        qp_b = -h\n",
    "        meq = 0\n",
    "    return quadprog.solve_qp(qp_G, qp_a, qp_C, qp_b, meq)[0]\n",
    "\n",
    "# Do not change the seed value\n",
    "np.random.seed(0) \n",
    "# Generate synthetic dataset\n",
    "X1 = np.round(np.random.randn(3, 2)).astype(int) + [2, 2]  # Class 1 (Shifted)\n",
    "X2 = np.round(np.random.randn(2, 2)).astype(int) + [-2, -2]  # Class 2 (Shifted)\n",
    "X = np.vstack((X1, X2))\n",
    "Y = np.hstack((-np.ones(3), np.ones(2)))  # Labels: 0 and 1\n",
    "\n",
    "# Plot the data distribution based on labels\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X[Y == -1][:, 0], X[Y == -1][:, 1], color='blue', label='Class -1')\n",
    "plt.scatter(X[Y == 1][:, 0], X[Y == 1][:, 1], color='red', label='Class 1')\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"Data Distribution by Label\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "We want to build an SVM model on the synthetic dataset: \n",
    "\n",
    "\n",
    "$x^{(1)} = (4, 2),\\ y^{(1)}=-1$\n",
    "\n",
    "$x^{(2)} = (3, 4),\\ y^{(2)}=-1$\n",
    "\n",
    "$x^{(3)} = (4, 1),\\ y^{(3)}=-1$\n",
    "\n",
    "$x^{(4)} = (-1, -2),\\ y^{(4)}=1$\n",
    "\n",
    "$x^{(5)} = (-2, -2),\\ y^{(5)}=1$\n",
    "    \n",
    "We need to solve the quadratic programming (QP) problem as the following form:\n",
    "\n",
    "$\n",
    "    \\min_{\\alpha}\\big( \\frac{1}{2}\\alpha^{T}Q\\alpha - (\\textbf{1})^{T}\\alpha \\big) \\\\\n",
    "    \\text{subject to: } y^{T}\\alpha=0,\\ \\alpha\\geq 0\n",
    "$\n",
    "\n",
    "The quadprog package by defaualt solves the QP as this form:\n",
    "\n",
    "$\n",
    "    \\min_{x}\\big( \\frac{1}{2}x^{T}Px + q^{T}x \\big) \\\\\n",
    "    \\text{subject to: } Gx\\leq h,\\ Ax = b\n",
    "$\n",
    "\n",
    "Therefore, in order to use quadprog, we need to establish the responding relationships between variables: \n",
    "$P=Q$, $q = -(\\textbf{1})^{T}$, $G = -(\\textbf{1})^{T}$, $h=(\\textbf{0})^{T}$, $A=y^T$, $b=(\\textbf{0})^{T}$\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Task 1: Compute matrix $Q$\n",
    "\n",
    "**9 points**\n",
    "\n",
    "First, we need to use $x^{(i)}$ and $y^{(i)}$ to compute matrix $Q$:\n",
    "\n",
    "$\n",
    "    Q = \\begin{bmatrix}\n",
    "    y^{(1)}y^{(1)}x^{(1)T}x^{(1)} & y^{(1)}y^{(2)}x^{(1)T}x^{(2)} & \\dots & y^{(1)}y^{(5)}x^{(1)T}x^{(5)} \\\\\n",
    "    y^{(2)}y^{(1)}x^{(2)T}x^{(1)} & y^{(2)}y^{(2)}x^{(2)T}x^{(2)} & \\dots & y^{(2)}y^{(5)}x^{(2)T}x^{(5)} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    y^{(5)}y^{(1)}x^{(5)T}x^{(1)} & y^{(5)}y^{(2)}x^{(5)T}x^{(2)} & \\dots & y^{(5)}y^{(5)}x^{(5)T}x^{(5)} \\\\\n",
    "    \\end{bmatrix}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros((5, 5))\n",
    "\n",
    "### START YOUR CODE ###\n",
    "for i in range(Q.shape[0]):\n",
    "    for j in range(Q.shape[1]):\n",
    "        # Use the ith and jth examples in X and Y to compute Q_ij\n",
    "        # Hint: Q_ij = y^i * y^j * (x^i @ x^j)\n",
    "        Q[i, j] = None\n",
    "### END YOUR CODE ###\n",
    "print('Q = ', Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "\n",
    "**Q =**|[[ 20. 20. 18.  8. 12.] <br>[ 20. 25. 16. 11. 14.] <br> [ 18. 16. 17.  6. 10.] <br> [ 8. 11.  6.  5.  6.] <br> [ 12. 14. 10.  6.  8.]]\n",
    "\n",
    "---\n",
    "### Task 2: Computer other variables\n",
    "**9 points**\n",
    "\n",
    "Use the folumas: $P=Q$, $q = -(\\textbf{1})^{T}$, $G = -(\\textbf{1})^{T}$, $h=(\\textbf{0})^{T}$, $A=y^T$, $b=(\\textbf{0})^{T}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "P = Q + np.eye(5)*1e-5 # To solve the non-positive finite issue\n",
    "\n",
    "# Hint: Use np.ones(), q is of length 5\n",
    "q =  None\n",
    "\n",
    "# Hint: G is a matrix whose diagnal elements are 1s, and other elements are 0s. Use np.eye()\n",
    "G = None\n",
    "\n",
    "# Hint: h is of length 5, with all zeros; Use np.zeros()\n",
    "h = None\n",
    "\n",
    "A = Y.reshape((1,5))\n",
    "\n",
    "# Hint: b is of length 1, with zero value; Use np.zeros()\n",
    "b = None\n",
    "\n",
    "### END YOUR CODE ###\n",
    "\n",
    "print('q = ', q)\n",
    "print('G = ', G)\n",
    "print('h = ', h)\n",
    "print('b = ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "\n",
    "**q =**  [-1. -1. -1. -1. -1.]\n",
    "**G =** [[-1. -0. -0. -0. -0.]<br> [-0. -1. -0. -0. -0.]<br> [-0. -0. -1. -0. -0.]<br> [-0. -0. -0. -1. -0.]<br> [-0. -0. -0. -0. -1.]]\n",
    "**h =**  [0. 0. 0. 0. 0.]\n",
    "**b =**  [0.]\n",
    "\n",
    "---\n",
    "\n",
    "### Task 3: Call quadprog\n",
    "**3 point**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START YOUR CODE ###\n",
    "\n",
    "# Hint: Call quadprog_solve_qp() with the correct arguments\n",
    "solution = None\n",
    "\n",
    "### END YOUR CODE ###\n",
    "\n",
    "print('solution = ', solution)\n",
    "print('The support vectors are: ', X[solution > 0, ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "\n",
    "**solution $\\approx$**|[0 0  0.06  0.06\n",
    " 0]\n",
    "**The support vectors are:** | [[4  1]<br> [-1 -2]]<br>\n",
    "\n",
    "---\n",
    "## Task 4: Solve the decision boundary\n",
    "**9 points**\n",
    "\n",
    "Use the support vectors to solve the $w$ and $b$ in the decision boundary $w^Tx+b=0$. Use the property that a support vector $x^{(k)}$ must satistify $y^{(k)}(w^Tx^{(k)}+b) = 1$. You can solve it with a paper and pen by listing linear equations.\n",
    "\n",
    "**NOTE**: Solve this task on paper. You only need to provide the answers for `w1`, `w2`, and `b`. Please use the approximated soulution for calculating $\\alpha$'s (i.e., solution).\n",
    "\n",
    "*Hint*: You should solve the following linear equations:\n",
    "\n",
    "$\\begin{cases} y^{(3)}(w^Tx^{(3)}+b) = 1 \\\\ y^{(4)}(w^Tx^{(4)}+b) = 1 \\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### START YOUR ANSWERS ###\n",
    "w1 = None\n",
    "w2 = None\n",
    "b = None\n",
    "### END YOUR ANSWERS\n",
    "\n",
    "print('w1 = ', w1)\n",
    "print('w2 = ', w2)\n",
    "print('b = ', b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS549",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
