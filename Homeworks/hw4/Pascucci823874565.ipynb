{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS549 Machine Learning  \n",
    "## Assignment 4\n",
    "\n",
    "**Total: 100 points**\n",
    "- Section 1: 30 points\n",
    "- Section 2: 30 points\n",
    "- Section 3: 40 points\n",
    "\n",
    "In this assignment, you will implement 1) the forward pass for a basic recurrent neural network, 2) decision tree and ensembles, and 3) k-means algirithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Character-Level RNN-Based Text Classification Model\n",
    "In this section, you will implement character-level RNN models that are used to classify the nationality of names.\n",
    "This is similar to the demo we showed in class :D\n",
    "\n",
    "This assignment is based on a tutorial from the PyTorch website: <https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html>.\n",
    "Some helper functions are adopted from the original example. \n",
    "\n",
    "The major difference is that you will use the `torch.nn.RNNCell` provided in PyTorch as your basic building block, instead of writing a RNN cell yourself from scratch.\n",
    "\n",
    "This section includes the following coding tasks:\n",
    "- Task 1.1 (6 pts): Defining the network\n",
    "- Task 1.2 (10 pts): Training the model\n",
    "- Task 1.3 (14 pts): Bi-directional RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from rnn2_utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "print('num of classes:', n_categories)\n",
    "print('num of letters:', n_letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.1 Defining the network\n",
    "\n",
    "**6 pts**\n",
    "\n",
    "Define a single directional RNN model.\n",
    "\n",
    "**Instructions**\n",
    "- An RNN cell `rnn_cell`, a fully-connected layer `self.fc`, and a softmax layer `self.softmax` are defined in `__init__()`\n",
    "- The computational graph is defined in `forward()`. The hidden state of the RNN cell is stored in the tensor `h`, which is of size `[batch_size, hidden_size]`. `forward()` takes as input the tensor `x` in of size `[seq_length, batch_size, input_size]`, whose first dimension is the time steps. Therefore, we use a `for` loop to iterate over the time steps, and for each time step, we feed the input at the current step, and the previous hidden state to the RNN cell. The cell then returns a new hidden state, which overrides `h`.\n",
    "- Refer to the original tutorial and the API for `nn.RNNCell` for detailed information: https://pytorch.org/docs/stable/nn.html#rnncell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: size [seq_length, 1, input_size]\n",
    "        \"\"\"\n",
    "        h = torch.zeros(x.size(1), self.hidden_size)\n",
    "        \n",
    "        for i in range(x.size(0)):\n",
    "            ### START YOUR CODE ### 2pts each\n",
    "            h = self.rnn_cell(None, None)\n",
    "            ### END YOUR CODE ###\n",
    "        \n",
    "        ### START YOUR CODE ### 2pts \n",
    "        # Hint: first call fc, then call softmax\n",
    "        out = None\n",
    "        ### END YOUR CODE ###\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Task 1.1\n",
    "### DO NOT CHANGE THE CODE BELOW ###\n",
    "torch.manual_seed(0)\n",
    "rnn = RNN(10, 20, 18)\n",
    "input_data = torch.randn(6, 3, 10)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = rnn(input_data)\n",
    "    \n",
    "print(out.size())\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "|&nbsp;|&nbsp;|\n",
    "|--|--|\n",
    "|torch.Size([3, 18])|\n",
    "|tensor([-3.3146, -3.0216, -2.9363, -3.1215, -2.7141, -2.7103, -2.9526, -3.0657,|\n",
    "| -2.9442, -3.0038, -2.6818, -2.7880, -2.9849, -3.1164, -2.5659, -2.5336,|\n",
    "| -2.9586, -2.9764])|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2 Training the model\n",
    "\n",
    "**10 pts**\n",
    "\n",
    "Train the model with stachastic gradient descent. Due to the setting of the origianl tutorrial, one data example is used per iteration, i.e., the mini-batch size is 1.\n",
    "\n",
    "**Instructions**\n",
    "- Each training data example contains a name (`x`) and a label indicating its nationality (`y`). \n",
    "- The model uses the tensor version of `x` and `y`, i.e., `x_tensor` and `y_tensor`. Here `x_tensor` is of size `[seq_length, 1, input_size]`, in which `input_size` is the total number of letters.\n",
    "- `x_tensor[0, 0, :]` as the one-hot encoding for the first letter in the string `x`, and so forth for `x_tensor[1, 0, :]` etc.\n",
    "- The function will output the predicing result every 5000 iterations. You can find a general trend that the prediction is getting more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, n_iters = 100000, print_every = 5000, plot_every = 1000, learning_rate = 0.005):\n",
    "    # Turn on the training model\n",
    "    model.train()\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "    \n",
    "    running_loss = 0\n",
    "    all_losses = []\n",
    "    \n",
    "    # Train loop\n",
    "    start = time.time()\n",
    "    for i in range(n_iters):\n",
    "        y, x, y_tensor, x_tensor = randomTrainingExample()\n",
    "        \n",
    "        ### START YOUR CODE ###\n",
    "        # zero grad 2pts\n",
    "        optimizer.None\n",
    "        \n",
    "        # Forward pass, 2pts each\n",
    "        output = None\n",
    "        loss = None\n",
    "        \n",
    "        # Backprop and update, 2pts each\n",
    "        loss.None\n",
    "        optimizer.None\n",
    "        ### END YOUR CODE ###\n",
    "        \n",
    "        # Record loss\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Print iter, loss, name, and guess\n",
    "        if i % print_every == 0 and i > 0:\n",
    "            guess, guess_i = categoryFromOutput(output)\n",
    "            correct = '✓' if guess == y else '✗ (%s)' % y\n",
    "            print('%d %d%% (%s) %.4f %s / %s %s' % (i, i / n_iters * 100, timeSince(start), loss, x, guess, correct))\n",
    "        \n",
    "        # Append loss\n",
    "        if i % plot_every == 0 and i > 0:\n",
    "            all_losses.append(running_loss / plot_every)\n",
    "            running_loss = 0\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure()\n",
    "    plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Task 1.2\n",
    "### DO NOT CHANGE THE CODE BELOW ###\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)\n",
    "\n",
    "train(rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "|&nbsp;|&nbsp;|\n",
    "|--|--|\n",
    "|5000 5% (0m 8s) | 3.1065 Ba / Vietnamese ✗ (Arabic)|\n",
    "|10000 10% (0m 17s)| 1.9009 Silva / Spanish ✗ (Portuguese)|\n",
    "|15000 15% (0m 27s)| 0.9776 Salucci / Italian ✓|\n",
    "|20000 20% (0m 36s)| 1.5460 Chaim / Korean ✗ (Chinese)|\n",
    "|25000 25% (0m 47s)| 1.9412 Cruz / Spanish ✗ (Portuguese)|\n",
    "|30000 30% (0m 57s)| 2.6189 Neusser / Dutch ✗ (Czech)|\n",
    "|35000 35% (1m 8s) |0.4944 Ribeiro / Portuguese ✓|\n",
    "|40000 40% (1m 18s)| 0.3007 Daher / Arabic ✓|\n",
    "|45000 45% (1m 28s)| 1.5447 Guang / Vietnamese ✗ (Chinese)|\n",
    "|50000 50% (1m 38s)| 0.9920 Mackenzie / Scottish ✓|\n",
    "|55000 55% (1m 48s)| 2.6635 Moon / English ✗ (Korean)|\n",
    "|60000 60% (1m 59s)| 2.5376 Boutros / Portuguese ✗ (Arabic)|\n",
    "|65000 65% (2m 9s) |0.5595 Schuttmann / German ✓|\n",
    "|70000 70% (2m 19s)| 1.6059 Spada / Japanese ✗ (Italian)|\n",
    "|75000 75% (2m 30s)| 1.1130 Coilean / Irish ✓|\n",
    "|80000 80% (2m 40s)| 0.6820 Schneider / German ✓|\n",
    "|85000 85% (2m 50s)| 0.3628 Lopez / Spanish ✓|\n",
    "|90000 90% (3m 1s) |1.1985 Tremblay / French ✓|\n",
    "|95000 95% (3m 12s)| 1.0904 Knopf / German ✓|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.3 Bi-directional RNN model\n",
    "**14 pts**\n",
    "\n",
    "Implement a bi-directional RNN model by modifying the architecture of the `RNN` class.\n",
    "\n",
    "**Instructions**\n",
    "- Two distinct RNN cells are defined in `__init__()`. The input size for the fully-connected layer is doubled, because it needs take input the concatenated hidden states from the two cells.\n",
    "- In `forward()`, two hidden states, `h1` and `h2`, are computed separately. `h1` is exactly the same as the previous `h` in one-directional `RNN`. `h2` is computed by reversing the order of the `for` loop: iterating from the last time step to the first time step.\n",
    "- Concatenate `h1` and `h2` using `torch.cat()`. Notice that the correct dimension needs be specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnn_cell1 = nn.RNNCell(input_size, hidden_size)\n",
    "        self.rnn_cell2 = nn.RNNCell(input_size, hidden_size)\n",
    "        \n",
    "        self.fc = nn.Linear(2 * hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: size [seq_length, 1, input_size]\n",
    "        \"\"\"\n",
    "        ### START YOUR CODE ### 2pts each\n",
    "        h1 = torch.zeros(x.size(1), self.hidden_size)\n",
    "        for i in range(x.size(0)):\n",
    "            h1 = self.rnn_cell1(None, None)\n",
    "        \n",
    "        h2 = torch.zeros(x.size(1), self.hidden_size)\n",
    "        for i in None: # 2pts each Hint: reverse the order of the for loop\n",
    "            h2 = self.rnn_cell2(None, None)\n",
    "        \n",
    "        h = None\n",
    "        out = None\n",
    "        ### END YOUR CODE ###\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Task 1.3\n",
    "### DO NOT CHANGE THE CODE BELOW ###\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "n_hidden = 128\n",
    "birnn = BiRNN(n_letters, n_hidden, n_categories)\n",
    "\n",
    "train(birnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "|&nbsp;|&nbsp;|\n",
    "|--|--|\n",
    "|5000 5% (0m 16s) | 2.9060 Ba / Vietnamese ✗ (Arabic)\n",
    "|10000 10% (0m 33s)| 1.9203 Silva / Spanish ✗ (Portuguese)\n",
    "|15000 15% (0m 51s)| 0.8597 Salucci / Italian ✓\n",
    "|20000 20% (1m 10s)| 1.8664 Chaim / Vietnamese ✗ (Chinese)\n",
    "|25000 25% (1m 29s)| 2.5580 Cruz / Spanish ✗ (Portuguese)\n",
    "|30000 30% (1m 50s)| 3.2462 Neusser / Dutch ✗ (Czech)\n",
    "|35000 35% (2m 8s) |0.4040 Ribeiro / Portuguese ✓\n",
    "|40000 40% (2m 27s)| 0.3474 Daher / Arabic ✓\n",
    "|45000 45% (2m 45s)| 1.1283 Guang / Vietnamese ✗ (Chinese)\n",
    "|50000 50% (3m 3s) |1.4291 Mackenzie / Russian ✗ (Scottish)\n",
    "|55000 55% (3m 20s)| 3.2796 Moon / Scottish ✗ (Korean)\n",
    "|60000 60% (3m 38s)| 2.5028 Boutros / Portuguese ✗ (Arabic)\n",
    "|65000 65% (3m 55s)| 0.0673 Schuttmann / German ✓\n",
    "|70000 70% (4m 13s)| 1.3614 Spada / Italian ✓\n",
    "|75000 75% (4m 30s)| 0.4321 Coilean / Irish ✓\n",
    "|80000 80% (4m 47s)| 1.7251 Schneider / Dutch ✗ (German)\n",
    "|85000 85% (5m 4s) |0.3610 Lopez / Spanish ✓\n",
    "|90000 90% (5m 22s)| 1.3607 Tremblay / French ✓\n",
    "|95000 95% (5m 40s)| 1.6964 Knopf / Czech ✗ (German)\n",
    "\n",
    "Simply from the above output, it seems that BiRNN does not bring significant improvement.\n",
    "Though we need more systematic evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task for fun! \n",
    "Try with the following functions on your own names!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(x_tensor)\n",
    "    return output\n",
    "\n",
    "\n",
    "def predict(model, input_line, n_predictions=3):\n",
    "    print('\\n> %s' % input_line)\n",
    "    with torch.no_grad():\n",
    "        output = evaluate(model, lineToTensor(input_line))\n",
    "\n",
    "        # Get top N categories\n",
    "        topv, topi = output.topk(n_predictions, 1, True)\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(n_predictions):\n",
    "            value = topv[0][i].item()\n",
    "            category_index = topi[0][i].item()\n",
    "            print('(%.2f) %s' % (value, all_categories[category_index]))\n",
    "            predictions.append([value, all_categories[category_index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with your name\n",
    "predict(birnn, 'Joann')\n",
    "predict(rnn, 'Joann')\n",
    "\n",
    "predict(birnn, 'Chen')\n",
    "predict(rnn, 'Chen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Decision tree and ensemble methods\n",
    "**30 pts**\n",
    "\n",
    "In this section, you will implement and evaluate a Decision Tree, Bagging, and Random Forest classifier.\n",
    "\n",
    "If needed, use `!pip install mlxtend` to install the package.\n",
    "\n",
    "This section includes the following coding tasks:\n",
    "- Task 2.1 (10 pts):  Run Decision tree classifiers with various parameters\n",
    "- Task 2.2 (8 pts): Q&A\n",
    "- Task 2.3 (6 pts): Bagging\n",
    "- Task 2.4 (6 pts): Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from sklearn.tree import plot_tree\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "feature_names = data.feature_names\n",
    "target_names = data.target_names\n",
    "print(feature_names)\n",
    "print(target_names) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1 Decision Tree Implementation\n",
    "**10pts**\n",
    "\n",
    "**Instructions**:\n",
    "1. Implement a tree using \"Gini impurity\" with a maximum tree depth of 1 (excluding the root node).\n",
    "2. Implement a tree using only two features (`sepal length (cm)` and `sepal width (cm)`) with \"Gini impurity\" as the criterion and a maximum tree depth of 3.\n",
    "3. Implement a tree using \"Entropy\" with a maximum tree depth of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Implement a tree using \"Gini impurity\" with a maximum tree depth of 1 (excluding the root node).\n",
    "### START YOUR CODE ### 2 pts\n",
    "dt1 = DecisionTreeClassifier(random_state=42, None)\n",
    "### END YOUR CODE ###\n",
    "dt1.fit(X_train, y_train)\n",
    "y_pred1 = dt1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Implement a tree using only two features (sepal length (cm) and sepal width (cm)) with \"Gini impurity\" as the criterion and a maximum tree depth of 3.\n",
    "### START YOUR CODE ### 2pts each\n",
    "dt2 = DecisionTreeClassifier(random_state=42, None)\n",
    "dt2.fit(X_train[None], y_train)\n",
    "y_pred2 = dt2.predict(X_test[None])\n",
    "### END YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Implement a tree using \"Entropy\" with a maximum tree depth of 3.\n",
    "### START YOUR CODE ### 2pts\n",
    "dt3 = DecisionTreeClassifier(random_state=42, None)\n",
    "### END YOUR CODE ###\n",
    "dt3.fit(X_train, y_train)\n",
    "y_pred3 = dt3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Task 2.1\n",
    "### DO NOT CHANGE THE CODE BELOW ###\n",
    "plot_tree(dt1, feature_names=feature_names, class_names=target_names, filled = True)\n",
    "print(\"Decision Tree #1 Accuracy:\", accuracy_score(y_test, y_pred1))\n",
    "plt.show()\n",
    "plot_tree(dt2, feature_names=feature_names[0:2], class_names=target_names, filled = True)\n",
    "print(\"Decision Tree #2 Accuracy:\", accuracy_score(y_test, y_pred2))\n",
    "plt.show()\n",
    "plot_tree(dt3, feature_names=feature_names, class_names=target_names, filled = True)\n",
    "print(\"Decision Tree #3 Accuracy:\", accuracy_score(y_test, y_pred3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "|&nbsp;|&nbsp;|\n",
    "|--|--|\n",
    "|**Decision Tree #1 Accuracy**:| 0.7111111111111111|\n",
    "|**Decision Tree #2 Accuracy**:| 0.7555555555555555 |\n",
    "|**Decision Tree #3 Accuracy**:| 0.9777777777777777|\n",
    "\n",
    "<img src=\"dt1.png\"><img src=\"dt2.png\"><img src=\"dt3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2 Q&A\n",
    "**8 pts**\n",
    "\n",
    "### What factors can impact the performance of a decision tree? List at least four."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here: _______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.3 Bagging Implementation\n",
    "**6 pts**\n",
    "\n",
    "**Instruction:**\n",
    "Implement Bagging with **100** estimators, using only two features (`sepal length (cm)` and `sepal width (cm)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging Implementation\n",
    "### START YOUR CODE ### 2pts each\n",
    "bagging = BaggingClassifier(None, random_state=42)\n",
    "bagging.fit(X_train[None], y_train)\n",
    "y_pred = bagging.predict(X_test[None])\n",
    "### START YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Task 2.3\n",
    "### DO NOT CHANGE THE CODE BELOW ###\n",
    "print(\"Bagging Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected output: \n",
    "Bagging Accuracy: 0.6888888888888889"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.4 Random Forest Implementation\n",
    "**6 pts**\n",
    "\n",
    "**Instruction:**\n",
    "Implement Random Forest with **10** estimators, using only two features (`sepal length (cm)` and `sepal width (cm)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Implementation\n",
    "### START YOUR CODE ### 2pts each\n",
    "rf = RandomForestClassifier(None, random_state=42)\n",
    "rf.fit(X_train[None], y_train)\n",
    "y_pred = rf.predict(X_test[None])\n",
    "### START YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Task 2.4\n",
    "### DO NOT CHANGE THE CODE BELOW ###\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected output: \n",
    "Random Forest Accuracy: 0.7333333333333333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: *K*-means clustering algorithm on image data\n",
    "\n",
    "**40pts**\n",
    "\n",
    "In this assignment, you will practice implementing K-means clustering, and then apply it on a subset of sign language dataset.\n",
    "\n",
    "The `PCA` module provided by `sklearn` package will be used for pre-clustering analysis and post-clustering visualization. You can install `sklearn` by running the following command in terminal:\n",
    "\n",
    "```\n",
    "!pip install scikit-learn\n",
    "```\n",
    "This section includes the following coding tasks:\n",
    "- Task 3.1 (3 pts): PCA\n",
    "- Task 3.2 (4 pts): Initialize centroids\n",
    "- Task 3.3 (8 pts): Compute distance between data points and centroids\n",
    "- Task 3.4 (7 pts): Find the closest centroid for each data point\n",
    "- Task 3.5 (8 pts): Update centroids\n",
    "- Task 3.6 (8 pts): Integrated model\n",
    "- Task 3.7 (2 pts): Visualize clustering result using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "np.random.seed(1)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Load data and preprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = np.load(open('X_train.npy', 'rb'))\n",
    "\n",
    "X1 = X_all[:163, :]\n",
    "X2 = X_all[163:327, :]\n",
    "X3 = X_all[327:491, :]\n",
    "\n",
    "X = np.concatenate((X1, X2, X3), axis=0)\n",
    "X = np.transpose(X, (1,2,3,0)).reshape(-1, X.shape[0])\n",
    "print('Shape of X:', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.1 PCA\n",
    "\n",
    "**3 points**\n",
    "\n",
    "First, we reduce and dimension of the original data to 2, and plot it. The goal of this step is to have some clues of what $k$ values to use, i.e., the number of clusters.\n",
    "\n",
    "We know that image data of 3 classes are selected, but we use them as if they are unlabelled. Judging from the 2-D plot, there are quite amount of outliers in data, and choosing $k=3$ may well group those outliers into a cluster, instead of grouping into the correct classes. For this we will find out at the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=None) # 1pt\n",
    "pca.fit(X.T)\n",
    "## Todo: Get transformed X values 2pts\n",
    "X_proj = None \n",
    "\n",
    "# Plot\n",
    "fig = plt.figure()\n",
    "plt.scatter(X_proj[:,0], X_proj[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "<img src=\"pca1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.2 Initialize centroids\n",
    "**4 pts**\n",
    "\n",
    "The first step of k-means is to randomly initialize a set of centroids. To accomplish this, we simply select $k$ out of the $m$ data points randomly.\n",
    "\n",
    "**Instructions:**\n",
    "- Data are stored in columns in `X`. We draw `k` random columns out of it by calling `numpy.random.choice()`. Notice that use the argument `replace=False` is important. Otherwise, it is possible to sample repeated points.\n",
    "- The returned `centroids` are in shape (n, k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize centroids\n",
    "def init_centroids(X, k):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    X -- data, shape (n, m)\n",
    "    k -- number of clusters\n",
    "    \n",
    "    Return:\n",
    "    centroids -- k randomly picked data points as initial centroids, shape (n, k)\n",
    "    \"\"\"\n",
    "    assert(k > 1)\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    ### START YOUR CODE ###\n",
    "    centroids = None\n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Task 3.2\n",
    "### DO NOT CHANGE THE CODE BELOW ###\n",
    "np.random.seed(1)\n",
    "X_tmp = np.random.randn(4, 10)\n",
    "c = init_centroids(X_tmp, k=3)\n",
    "\n",
    "print('Shape of centroids:', c.shape)\n",
    "print('centroids:', c)\n",
    "\n",
    "c_expanded = np.expand_dims(c, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "|&nbsp;|&nbsp;|\n",
    "|--|--|\n",
    "**Shape of centroids:**(4, 3)\n",
    "**centroids:**[[-0.52817175 -0.24937038  1.74481176]<br>[-0.3224172   0.58281521 -0.17242821]<br>[ 0.90159072  0.53035547 -0.12289023]]<br>[-0.6871727   0.74204416 -1.11731035]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.3 Compute distance between data points and centroids\n",
    "**8 points**\n",
    "\n",
    "Next, we need to compute the distances between data points and centroids. More concretely, for each data point `X[:,i]`, we need to compute its distance from the $k$ centroids, i.e., `centroids[:, j]` ($j=1,2,\\dots,k$). We will store the computed distances in a $k\\times m$ array, in which the element at position $(j, i)$ is the distance between `X[:,i]` and `centroids[:,j]`. The distance we talk about here is Euclidean distance.\n",
    "\n",
    "There are multiple ways of implementing this computation. The most efficient way is as follows:\n",
    "- First, expand `centroids` by adding one demension to it, so that its shape changes from (n, k) to (n, 1, k). This can be done by calling `np.expand_dims()`.\n",
    "- Second, transpose `X` and `centroids_expanded`. The former has shape (m, n) and the latter has shape (k, 1, n). Then the subtraction `S = X.T - centroids_expanded.T` will be in shape (k, m, n). For why it is the case, read the documentation about the broadcasting mechanism of numpy __[here](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.broadcast.html)__. \n",
    "Next, following the definition of Euclidean distance, we need to:\n",
    "    - Compute `S**2`, which is in shape (k, m, n).\n",
    "    - Sum over `S**2` along `axis=2`, which eliminate the last dimension.\n",
    "    - Apply `numpy.sqrt()` to `S**2`, resulting in an array of shape (k, m), which gives the Euclidean distances.\n",
    "\n",
    "If you found the above method hard to follow, you can also use an explicit for loop to do the computation.\n",
    "- You create an empty array `distances` of shape (k, m).\n",
    "- Then you use a for loop, `for j in range(k):`, and in each step, you compute `S = X - centroids[:,j]` followed by `S**2`, `numpy.sum()`, and `numpy.sqrt()` to get the Euclidean distance, which is stored in a (1,m) array `d`. Then you copy `d` back to the `j`th row of `distances`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute distances\n",
    "def compute_distances(X, centroids):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    X -- data, shape (n, m)\n",
    "    centroids -- shape (n, k)\n",
    "    \n",
    "    Return:\n",
    "    distances -- shape (k, m)\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ### 4 pts each\n",
    "    centroids_expanded = None\n",
    "    distances = None\n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Task 3.3\n",
    "### DO NOT CHANGE THE CODE BELOW ###\n",
    "np.random.seed(1)\n",
    "X_tmp = np.random.randn(4, 5)\n",
    "c = init_centroids(X_tmp, k=2)\n",
    "\n",
    "dists = compute_distances(X_tmp, c)\n",
    "print('Distances:', dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "\n",
    "**Distances:**[[3.19996571 3.13120276 0.         1.52120576 2.54127667]<br>[5.88553536 0.         3.13120276 2.25851302 4.11463616]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.4 Find the closest centroid for each data point\n",
    "**7 pts**\n",
    "\n",
    "Given the distances computed, we can find the closest centroid for each data point. We store this information in a $1\\times m$ array, and each element is the index of the closest centroid, i.e., an integer ranging from $0$ to $k-1$.\n",
    "\n",
    "**Instructions:**\n",
    "- You can apply `numpy.argmin()` on the `distances` computed in previous step as input, and a proper `axis` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the closest centroid for each data point\n",
    "def cloeset_centroid(distances):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    distances -- numpy array of shape (k, m), output of compute_distances()\n",
    "    \n",
    "    Return:\n",
    "    indices -- numpy array of shape (1, m)\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ###\n",
    "    indices = None\n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Task 3.4\n",
    "### DO NOT CHANGE THE CODE BELOW ###\n",
    "np.random.seed(1)\n",
    "X_tmp = np.random.randn(4, 5)\n",
    "c = init_centroids(X_tmp, k=2)\n",
    "\n",
    "dists = compute_distances(X_tmp, c)\n",
    "closest_indices = cloeset_centroid(dists)\n",
    "\n",
    "print('Indices of the cloest centroids:', closest_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "\n",
    "**Indices of the cloest centroids:**[0 1 0 0 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.5 Update centroids\n",
    "**8 pts**\n",
    "\n",
    "Given the indices of closest centroid for each data point, you need to update the centroids by computing the average positions of the data points belonging to each cluster ($1,2,\\dots,k$).\n",
    "\n",
    "**Instructions:**\n",
    "- Because `closest_indices` (output of `closes_centroid()`) is of shape (1, m), you can access the data points whose closest centroid is `j` by using the slice `X[:, closest_indices==j]`.\n",
    "- Pay attention to the dimension of `new_centroids` computed, and it needs to be the same as `centroids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update centroids\n",
    "def update_centroids(X, closest_indices, centroids):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    X -- data, shape (n, m)\n",
    "    cloesest_indices -- output of closest_centroid()\n",
    "    centroids -- old centroids positions\n",
    "    \n",
    "    Return:\n",
    "    new_centroids -- new centroids positions, shape (n, k)\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ### \n",
    "    new_centroids = None\n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    assert(centroids.shape == new_centroids.shape)\n",
    "    \n",
    "    return new_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Task 3.5\n",
    "### DO NOT CHANGE THE CODE BELOW ###\n",
    "np.random.seed(1)\n",
    "X_tmp = np.random.randn(4, 5)\n",
    "c = init_centroids(X_tmp, k=2)\n",
    "\n",
    "dists = compute_distances(X_tmp, c)\n",
    "closest_indices = cloeset_centroid(dists)\n",
    "new_c = update_centroids(X_tmp, closest_indices, c)\n",
    "\n",
    "print('New centroids:', new_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "|&nbsp;|&nbsp;|\n",
    "|--|--|\n",
    "**New centroids:**[[ 0.22215315 -0.61175641]<br>[-0.74826922  1.74481176]<br>[ 0.47235146 -2.06014071]<br>[-0.33818018 -0.17242821]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.6 Integrated model\n",
    "**8 pts**\n",
    "\n",
    "Finally, we combine all the previous steps into one model. We repeatedly find the closest centroid for each data points, and then update the centroids, until the centroids no longer change. The final stable `closest_indices` is then the clustering result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means\n",
    "def kmeans(X, k):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    X -- data, shape (n, m)\n",
    "    k -- number of clusters\n",
    "    \n",
    "    Return:\n",
    "    closest_indices -- final assignment of clusters to each data point, shape (1, m)\n",
    "    centroids -- final positions of centroids\n",
    "    \"\"\"\n",
    "    ### START YOUR CODE ### 2pts\n",
    "    centroids = None\n",
    "    ### END YOUR CODE ###\n",
    "    \n",
    "    old_centroids = None ## this is not a todo\n",
    "    while not np.array_equal(old_centroids, centroids):\n",
    "        # Backup centroids\n",
    "        old_centroids = np.copy(centroids)\n",
    "        \n",
    "        ### START YOUR CODE ### 2pts each\n",
    "        # Compute distances\n",
    "        distances = None\n",
    "        \n",
    "        # Find cloeset centroid\n",
    "        closest_indices = None\n",
    "        \n",
    "        # Update centroids\n",
    "        centroids = None\n",
    "        ### END YOUR CODE ###\n",
    "    \n",
    "    return closest_indices, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Task 3.6\n",
    "### DO NOT CHANGE THE CODE BELOW ###\n",
    "closest_indices, centroids = kmeans(X, 3)\n",
    "\n",
    "print('closest_indices[:10]', closest_indices[:10])\n",
    "print('closest_indices[70:80]', closest_indices[70:80])\n",
    "print('closest_indices[140:150]', closest_indices[140:150])\n",
    "print('closest_indices[210:220]', closest_indices[210:220])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "|&nbsp;|&nbsp;|\n",
    "|--|--|\n",
    "**closest_indices[:10]**[1 1 0 0 1 1 1 1 1 0]\n",
    "**closest_indices[70:80]**[1 1 1 1 1 1 1 1 1 1]\n",
    "**closest_indices[140:150]**[1 1 1 1 1 1 1 1 1 0]\n",
    "**closest_indices[210:220]**[2 1 1 1 2 2 2 2 2 2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3.7 Visualize clustering result using PCA\n",
    "\n",
    "**2pts**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=None) #1pt\n",
    "pca.fit(X.T)\n",
    "X_proj = None  #1pt\n",
    "\n",
    "# Cluster and \n",
    "fig = plt.figure(figsize=(12, 3.5))\n",
    "\n",
    "closest_indices, centroids = kmeans(X, 2)\n",
    "fig.add_subplot(1, 2, 1)\n",
    "plt.scatter(X_proj[closest_indices==0, 0], X_proj[closest_indices==0, 1])\n",
    "plt.scatter(X_proj[closest_indices==1, 0], X_proj[closest_indices==1, 1])\n",
    "plt.scatter(X_proj[closest_indices==2, 0], X_proj[closest_indices==2, 1])\n",
    "plt.title('Clustering result of k=2')\n",
    "\n",
    "closest_indices, centroids = kmeans(X, 3)\n",
    "fig.add_subplot(1, 2, 2)\n",
    "plt.scatter(X_proj[closest_indices==0, 0], X_proj[closest_indices==0, 1])\n",
    "plt.scatter(X_proj[closest_indices==1, 0], X_proj[closest_indices==1, 1])\n",
    "plt.scatter(X_proj[closest_indices==2, 0], X_proj[closest_indices==2, 1])\n",
    "plt.scatter(X_proj[closest_indices==3, 0], X_proj[closest_indices==3, 1])\n",
    "plt.title('Clustering result of k=3')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "<img src=\"kmean.png\" style=\"width:800px;height:300px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS549",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
